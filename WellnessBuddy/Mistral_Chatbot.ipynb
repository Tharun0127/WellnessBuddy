{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install accelerate peft bitsandbytes git+https://github.com/huggingface/transformers trl py7zr auto-gptq optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "data = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "data_df = data.to_pandas()\n",
    "data_df = data_df[:5000]\n",
    "data_df[\"text\"] = data_df[[\"input\", \"instruction\", \"output\"]].apply(lambda x: \"###Human: \" + x[\"instruction\"] + \" \" + x[\"input\"] + \" ###Assistant: \"+ x[\"output\"], axis=1)\n",
    "data = Dataset.from_pandas(data_df)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True, tokenizer=tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                            \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n",
    "                            quantization_config=quantization_config_loading,\n",
    "                            device_map=\"auto\"\n",
    "                        )\n",
    "\n",
    "\n",
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"mistral-finetuned-alpaca\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        max_steps=250,\n",
    "        fp16=True,\n",
    "        push_to_hub=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=data,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=training_arguments,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=False,\n",
    "        max_seq_length=512\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r /content/mistral-finetuned-alpaca /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import GenerationConfig\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/mistral-finetuned-alpaca\")\n",
    "\n",
    "inputs = tokenizer(\"\"\"###Human: Why mobile is bad for human? ###Assistant: \"\"\", return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"/content/mistral-finetuned-alpaca\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "st_time = time.time()\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(time.time()-st_time)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
